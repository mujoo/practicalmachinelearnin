---
title: "Practical Machine Learning - Project "
author: "Vishakha"
date: "Saturday, December 20, 2014"
output: html_document
---
## Synopsis ##

The goal of  project is to Create a model based on data provided and then use that model to predict  "classe" variable in the testing set. 
I have used Decision tree and Random forest to create model and found random forest model more accurate in this case 

## Selecting My algorithim ##

The predictor I finally  used for this classification problem is a random forest. The reasons I am employing a random forest are:

1. After filtering out multiple variables there are still 52 input variables to work with. Random forests are particularly well suited to handle a large number of inputs, especially when the interactions between variables are unknown.
2. I also tried Decision Tree but random forset accuracy was 99.36 where as Decision Tree it was 68.99
3 A random forest can handle unscaled variables and categorical variables, which reduces the need for cleaning and transforming variables which are steps that can be subject to overfitting and noise.

```{r}

# set working directory 
setwd("~/Rworkingdir/practicalmachinemlearning")
# Load Required Libraries 
library(caret)
library(randomForest)
library(rpart)
# there was error message when I tried to work with random forest and after loading this library everything worked 
library(e1071)
# reading training and test data set  

training <- read.table("pml-training.csv", header = TRUE, sep = ",")
testing <- read.table("pml-testing.csv", header = TRUE, sep = ",")
# remove the first 8 columns which are just identifier and may not help in prediction

training2 <- training[, -seq(from = 1, to = 8, by = 1)]
```

## Partioning the data set into two ## 

```{r}
inTrain <- createDataPartition(y=training2$classe, p=0.6, list=FALSE)
myTraining <- training2[inTrain, ]; myTesting <- training2[-inTrain, ]
dim(myTraining); dim(myTesting)
```
## More Cleaning The data
Cleaning NearZeroVariance Variables Run this code to view possible NZV Variables:
```{r}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
#Run this code to create another subset without NZV variables:
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
#To check the new N of observations
dim(myTraining)
```
## Cleaning variables with too many NA's

```{r}
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .8 ) { #if number of  NAs > 80% of total observations
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
                trainingV3 <- trainingV3[ , -j] #Remove that column
            }   
        } 
    }
}
#To check the new N of observations
dim(trainingV3)
myTraining <- trainingV3
rm(trainingV3)
```
Now let us do the exact same  transformations  for our myTesting and testing data sets.

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -52]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
```


```{r}
#To check the new N of observationsdim(myTesting)
dim(testing)
```
## USING DECISION TREE  FOR PREDICTION

```{r}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
#Using confusion Matrix to test results:
confusionMatrix(predictionsA1, myTesting$classe)
```
## Using RANDOM FOREST FOR PREDICTION

```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining)
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
#Using confusion Matrix to test results:
confusionMatrix(predictionsB1, myTesting$classe)
```
### Random Forests yielded better with Kappa 0.9919 and accuracy .9936 ###


##The out-of-bag (oob) error estimate##

In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:

Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.

## Generating Files to submit as answers for the Assignment:

Since  Random Forests  prediction was more accirate :
```{r}
predictionsB2 <- predict(modFitB1, testing, type = "class")

#Function to generate files with predictions to submit for assignment

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsB2)
```
